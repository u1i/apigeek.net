# AI力量覺醒

還記得OpenAI的GPT曾經是大多數AI項目的默認選擇嗎?現在,僅在幾個月內,AI領域就發生了翻天覆地的變化,Meta、Microsoft、Databricks和Apple等科技巨頭紛紛發布了自己的開放模型。讓我們來看看這個充滿選擇和易於獲取的新時代是如何改變我們構建智能應用的方式,為什麼沒有一刀切的解決方案,以及你如何在這個快速發展的環境中找到最適合你需求的方法。

直到去年年底,當被問及使用AI構建智能應用時,我不情願地承認,當時OpenAI的GPT可能是唯一可行的選擇。這就像有一個只有一種工具的工具箱——或者說有許多強大的工具,但都來自同一個品牌,有著相同的局限性和怪癖。我對GPT家族非常熟悉,使用過GPT-2、GPT-3以及所有的.5和turbo版本。不要誤會我的意思,它們很棒,但正如我所說,都是來自一家公司。我之前曾寫過關於需要一個B計劃的文章。當你常用的大型語言模型API突然被棄用並被新的取代時,會發生什麼?

## AI模型革命:多元生態系統的崛起

但是,僅在幾個月內,一切都變了。Anthropic和Google發布了幾乎與OpenAI一樣強大的先進AI模型。更重要的是,Meta、Apple和Microsoft等科技巨頭已經開源了他們的模型,並提供下載。這意味著你可以在自己的基礎設施上運行它們,而不僅僅是通過API訪問,有些人對此感到不舒服,因為這涉及將你的數據發送到未知的地方。

突然間,我的工具箱裡塞滿了閃閃發光的新工具,每一個都有自己獨特的特性和能力,AI模型領域已經轉變為一個多元化的生態系統。其中一些新的開放模型,如Llama 3,現在與僅僅1.5年前的GPT一樣強大,而且令人驚訝的是,有些甚至可以在我的MacBook Air上運行!這就像看到一個黑白世界突然爆發出色彩,AI變得比以往任何時候都更容易獲取和更通用。讓我們來看看過去幾個月中一些最重要的發布。

## 大型語言模型的最新進展

• Google Gemini(2023年12月6日):Google的先進模型系列,包括1.0、1.5(Pro和Ultra)版本,上下文窗口高達100萬個token——我與這些模型進行了大量工作,真的很喜歡它們的發展方向。
• Mistral的Mixtral(2023年12月11日):發布Mixtral 8x7B,這是一個高質量的稀疏專家混合模型(SMoE),具有開放權重,在大多數標準基準測試中與GPT3.5相當或優於GPT3.5。在那之前,我還沒有太多探索Mistral模型。
• Google的Gemma(2024年2月21日):一系列輕量級、先進的開放模型,基於與Google的Gemini模型相同的研究和技術構建,使開發人員更容易獲得先進的AI。它們是非常小的模型,當我嘗試使用它們時,我意識到對於某些使用情況,我們真正需要的只是一個可以理解語言並與用戶互動的引擎。現在,我們可以將這些功能嵌入到我可以在自己的機器上運行的軟件中,而不需要依賴OpenAI。
• Anthropic Claude 3(2024年3月4日):Anthropic的最新產品包括Haiku、Sonnet和Opus,我很幸運能夠在一段時間內獲得這些模型的研究訪問權限。在我看來,它們是目前OpenAI模型最強大的替代品。特別是Opus,感覺溫暖、引人入勝,是一個了不起的模型,可以構建能夠進行深入智能對話並展示先進推理能力的數字人格。
• X的Grok(2024年3月18日發布):宣布為「開源語言模型」,該公司只向公眾發布了權重,沒有代碼或太多文檔,讓許多開發人員對其實際能力和可用性感到不確定。我很想嘗試一下,但它現在太大了,無法在我自己的硬件或雲端運行。
• Apple的MM1(2024年3月19日發布):一個多模態大型語言模型,清楚地表明Apple正在認真對待AI。我還沒有機會嘗試它。
• Databricks的DBRX(2024年3月27日):一個開放模型,引入了細粒度的專家混合(MoE)架構,以比GPT更少的參數實現了令人印象深刻的性能。這對企業公司來說可能非常有趣,因為它使他們能夠在自己的基礎設施上運行強大的AI模型。Databricks對企業需求的關注,如數據合規性和針對特定使用情況微調模型的能力,使DBRX成為許多企業的一個有吸引力的選擇。
• Meta的Llama 3(2024年4月18日):Meta最新的開放模型,在海量數據集上進行了訓練,提供了與僅僅1.5年前的GPT相當的性能。我對輸出的質量、它對自定義提示的反應以及它的多功能性和可用性印象深刻。當然,我之前見過LLama 2,但沒有太多使用。Facebook、WhatsApp和Instagram現在都內置了LLama 3驅動的功能。當你想到現在對AI公司來說,獲得新鮮數據來訓練他們的模型是一個巨大的問題時,想想這些平台擁有多少億用戶。Meta處於一個非常有利的位置。人類反饋的強化學習是使AI模型更智能和創造下一代的關鍵。
• Microsoft的Phi-3(2024年4月23日):一系列專為效率和性能而設計的小型語言模型(SLM),特別適合邊緣計算和離線場景。我很喜歡我與Phi-3模型的第一組互動,我相信它們將在優先考慮速度、隱私和設備功能而非原始能力的應用中找到一個強大的利基市場。

這個時間表顯示了創新的快速步伐和AI模型環境在短短幾個月內日益增長的多樣性。每個版本都帶來了新的功能、架構和潛在的使用情況,為開發人員和企業提供了豐富的選擇。

## OpenAI和更高效AI架構的崛起

但是,OpenAI在這一切中處於什麼位置?這家運營超級知名ChatGPT並通過其GPT模型主導大部分AI領域的公司在最近的發布熱潮中明顯缺席(如果你暫時忽略SORA公告的話)。雖然他們可能正在幕後開發GPT-5,但很明顯,一些較新的架構,特別是那些使用專家混合(MoE)技術的架構,正開始挑戰GPT的霸主地位。

像DBRX和Llama 3這樣的模型已經證明,通過更少的參數和更高效的架構,可以實現令人印象深刻的性能。而向MoE模型的轉變,可以在更小的基礎設施要求下提供更好的結果,有效地蠶食了OpenAI的午餐,侵蝕了GPT曾經擁有的競爭優勢。

那麼,我們如何在這個AI模型選擇和可及性的新時代中找到最適合我們需求的方法?

## 利用AI模型的多樣性

當我們在這個AI模型選擇和可及性的新時代中探索時,重要的是要認識到沒有一刀切的解決方案。正如我們的大腦為不同的任務使用不同的系統,從自動和無意識的(如系鞋帶)到深思熟慮和分析的(如解決一個複雜的數學問題),我們可以針對特定目的利用不同的AI模型。

在他的著作《思考,快與慢》中,心理學家Daniel Kahneman介紹了我們頭腦中兩個不同系統的概念:系統1,自動快速運作;系統2,將注意力分配給更費力的心智活動。我們可以將這個框架應用於AI模型的世界,將較小、更高效的模型用於需要快速反應和最少計算資源的任務(類似於系統1),將較大、更複雜的模型用於需要更深入推理和分析的任務(類似於系統2)。

此外,我們可以結合在特定任務中表現出色的模型,創建強大的、多面的AI系統。例如,我們可能會將Phi-3這樣的模型用於邊緣計算和離線場景,將Gemma用於輕量級的設備上處理,將DBRX或Llama 3用於更複雜的基於雲的任務。通過了解每個模型的優缺點並將它們戰略性地結合起來,我們可以構建更高效、更有效,並能適應廣泛使用情況的AI應用。

## 你可以使用靈活、有見地的方法

最終,在這個新環境中取得成功的關鍵是以靈活、有見地的心態來選擇和部署AI模型。通過及時了解最新進展、嘗試不同的模型,並仔細考慮每個項目的具體需求和限制,開發人員和企業可以利用這個AI新時代的力量,同時避免過度依賴任何單一模型或提供商的陷阱。

這是AI的激動人心的時刻,在我們向前邁進時,讓我們擁抱AI模型環境的多樣性和可及性,因為這顯然代表著人工智能民主化和進步的重要一步。