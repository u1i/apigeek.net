# AI力量的觉醒

还记得OpenAI的GPT曾经是大多数AI项目的默认选择吗?现在,仅在几个月内,AI领域就发生了翻天覆地的变化,Meta、Microsoft、Databricks和Apple等科技巨头纷纷发布了自己的开放模型。让我们来看看这个充满选择和易于获取的新时代是如何改变我们构建智能应用的方式,为什么没有一刀切的解决方案,以及你如何在这个快速发展的环境中找到最适合你需求的方法。

直到去年年底,当被问及使用AI构建智能应用时,我不情愿地承认,当时OpenAI的GPT可能是唯一可行的选择。这就像有一个只有一种工具的工具箱——或者说有许多强大的工具,但都来自同一个品牌,有着相同的局限性和怪癖。我对GPT家族非常熟悉,使用过GPT-2、GPT-3以及所有的.5和turbo版本。不要误会我的意思,它们很棒,但正如我所说,都是来自一家公司。我之前曾写过关于需要一个B计划的文章。当你常用的大型语言模型API突然被弃用并被新的取代时,会发生什么?

## AI模型革命:多元生态系统的崛起

但是,仅在几个月内,一切都变了。Anthropic和Google发布了几乎与OpenAI一样强大的先进AI模型。更重要的是,Meta、Apple和Microsoft等科技巨头已经开源了他们的模型,并提供下载。这意味着你可以在自己的基础设施上运行它们,而不仅仅是通过API访问,有些人对此感到不舒服,因为这涉及将你的数据发送到未知的地方。

突然间,我的工具箱里塞满了闪闪发光的新工具,每一个都有自己独特的特性和能力,AI模型领域已经转变为一个多元化的生态系统。其中一些新的开放模型,如Llama 3,现在与仅仅1.5年前的GPT一样强大,而且令人惊讶的是,有些甚至可以在我的MacBook Air上运行!这就像看到一个黑白世界突然爆发出色彩,AI变得比以往任何时候都更容易获取和更通用。让我们来看看过去几个月中一些最重要的发布。

## 大型语言模型的最新进展

• Google Gemini(2023年12月6日):Google的先进模型系列,包括1.0、1.5(Pro和Ultra)版本,上下文窗口高达100万个token——我与这些模型进行了大量工作,真的很喜欢它们的发展方向。
• Mistral的Mixtral(2023年12月11日):发布Mixtral 8x7B,这是一个高质量的稀疏专家混合模型(SMoE),具有开放权重,在大多数标准基准测试中与GPT3.5相当或优于GPT3.5。在那之前,我还没有太多探索Mistral模型。
• Google的Gemma(2024年2月21日):一系列轻量级、先进的开放模型,基于与Google的Gemini模型相同的研究和技术构建,使开发人员更容易获得先进的AI。它们是非常小的模型,当我尝试使用它们时,我意识到对于某些使用情况,我们真正需要的只是一个可以理解语言并与用户互动的引擎。现在,我们可以将这些功能嵌入到我可以在自己的机器上运行的软件中,而不需要依赖OpenAI。
• Anthropic Claude 3(2024年3月4日):Anthropic的最新产品包括Haiku、Sonnet和Opus,我很幸运能够在一段时间内获得这些模型的研究访问权限。在我看来,它们是目前OpenAI模型最强大的替代品。特别是Opus,感觉温暖、引人入胜,是一个了不起的模型,可以构建能够进行深入智能对话并展示先进推理能力的数字人格。
• X的Grok(2024年3月18日发布):宣布为"开源语言模型",该公司只向公众发布了权重,没有代码或太多文档,让许多开发人员对其实际能力和可用性感到不确定。我很想尝试一下,但它现在太大了,无法在我自己的硬件或云端运行。
• Apple的MM1(2024年3月19日发布):一个多模态大型语言模型,清楚地表明Apple正在认真对待AI。我还没有机会尝试它。
• Databricks的DBRX(2024年3月27日):一个开放模型,引入了细粒度的专家混合(MoE)架构,以比GPT更少的参数实现了令人印象深刻的性能。这对企业公司来说可能非常有趣,因为它使他们能够在自己的基础设施上运行强大的AI模型。Databricks对企业需求的关注,如数据合规性和针对特定使用情况微调模型的能力,使DBRX成为许多企业的一个有吸引力的选择。
• Meta的Llama 3(2024年4月18日):Meta最新的开放模型,在海量数据集上进行了训练,提供了与仅仅1.5年前的GPT相当的性能。我对输出的质量、它对自定义提示的反应以及它的多功能性和可用性印象深刻。当然,我之前见过LLama 2,但没有太多使用。Facebook、WhatsApp和Instagram现在都内置了LLama 3驱动的功能。当你想到现在对AI公司来说,获得新鲜数据来训练他们的模型是一个巨大的问题时,想想这些平台拥有多少亿用户。Meta处于一个非常有利的位置。人类反馈的强化学习是使AI模型更智能和创造下一代的关键。
• Microsoft的Phi-3(2024年4月23日):一系列专为效率和性能而设计的小型语言模型(SLM),特别适合边缘计算和离线场景。我很喜欢我与Phi-3模型的第一组互动,我相信它们将在优先考虑速度、隐私和设备功能而非原始能力的应用中找到一个强大的利基市场。

这个时间表显示了创新的快速步伐和AI模型环境在短短几个月内日益增长的多样性。每个版本都带来了新的功能、架构和潜在的使用情况,为开发人员和企业提供了丰富的选择。

## OpenAI和更高效AI架构的崛起

但是,OpenAI在这一切中处于什么位置?这家运营超级知名ChatGPT并通过其GPT模型主导大部分AI领域的公司在最近的发布热潮中明显缺席(如果你暂时忽略SORA公告的话)。虽然他们可能正在幕后开发GPT-5,但很明显,一些较新的架构,特别是那些使用专家混合(MoE)技术的架构,正开始挑战GPT的霸主地位。

像DBRX和Llama 3这样的模型已经证明,通过更少的参数和更高效的架构,可以实现令人印象深刻的性能。而向MoE模型的转变,可以在更小的基础设施要求下提供更好的结果,有效地蚕食了OpenAI的午餐,侵蚀了GPT曾经拥有的竞争优势。

那么,我们如何在这个AI模型选择和可及性的新时代中找到最适合我们需求的方法?

## 利用AI模型的多样性

当我们在这个AI模型选择和可及性的新时代中探索时,重要的是要认识到没有一刀切的解决方案。正如我们的大脑为不同的任务使用不同的系统,从自动和无意识的(如系鞋带)到深思熟虑和分析的(如解决一个复杂的数学问题),我们可以针对特定目的利用不同的AI模型。

在他的著作《思考,快与慢》中,心理学家Daniel Kahneman介绍了我们头脑中两个不同系统的概念:系统1,自动快速运作;系统2,将注意力分配给更费力的心智活动。我们可以将这个框架应用于AI模型的世界,将较小、更高效的模型用于需要快速反应和最少计算资源的任务(类似于系统1),将较大、更复杂的模型用于需要更深入推理和分析的任务(类似于系统2)。

此外,我们可以结合在特定任务中表现出色的模型,创建强大的、多面的AI系统。例如,我们可能会将Phi-3这样的模型用于边缘计算和离线场景,将Gemma用于轻量级的设备上处理,将DBRX或Llama 3用于更复杂的基于云的任务。通过了解每个模型的优缺点并将它们战略性地结合起来,我们可以构建更高效、更有效,并能适应广泛使用情况的AI应用。

## 你可以使用灵活、有见地的方法

最终,在这个新环境中取得成功的关键是以灵活、有见地的心态来选择和部署AI模型。通过及时了解最新进展、尝试不同的模型,并仔细考虑每个项目的具体需求和限制,开发人员和企业可以利用这个AI新时代的力量,同时避免过度依赖任何单一模型或提供商的陷阱。

这是AI的激动人心的时刻,在我们向前迈进时,让我们拥抱AI模型环境的多样性和可及性,因为这显然代表着人工智能民主化和进步的重要一步。